{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g-roger/natural-language-process/blob/main/FilmeReviewsTrabalhoFinal_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "Gabriel Roger do Nascimento - 340399 <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "###**Criar um classificador de sentimento aplicando técnicas de PLN**\n",
        "---\n",
        "\n",
        "Utilizando o dataset de revisões de filmes em português [1], criar um classificador de sentimentos que consiga um score na métrica F1 Score superior a 70%.\n",
        "\n",
        "Devem utilizar uma amostra de 20% e randon_state igual a 42 para testar as implementações e mensurar a métrica F1 Score (usar o parâmetro average = 'weighted') o restante dos dados devem ser utilizados para o treinamento (80%).\n",
        "\n",
        "Fique a vontade para testar os métodos de pré-processamento, abordagens, algoritmos e bibliotecas, mas explique e justifique suas decisões.\n",
        "O trabalho poderá ser feito em grupo de até 4 pessoas (mesmo grupo do Startup One).\n",
        "\n",
        "Separe a implementação do seu modelo campeão junto com a parte de validação/teste de forma que o professor consiga executar todo o pipeline do modelo campeão.\n",
        "\n",
        "Composição da nota:\n",
        "- 50% - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, etc.)\n",
        "- 50% - Baseado na performance obtida com o dataset de teste (conforme recomendação da amostra) no seu modelo campeão e na validação que o professor processar (Métrica F1 Score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzhQpodBpRpX"
      },
      "source": [
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMBI8SQtps1n"
      },
      "outputs": [],
      "source": [
        "# CARREGANDO O DATA FRAME\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv', encoding='utf-8')\n",
        "\n",
        "# Façam o download do arquivo e utilizem localmente durante os testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s__lBzDQwrcG"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "Bom desenvolvimento!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nze8UbKhosm9"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSG0THRFhEg8"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue0nV0uVo3OZ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6Ctaw4oMxQ7"
      },
      "source": [
        "Nós escolhemos SpaCy justamente por ter uma simplicidade e desempenho ótimos.\n",
        "\n",
        "Uma grande vantagem é por ter modelos treinados para o português."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FziwgqJmw9OD"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujpi3S5L8cAX"
      },
      "outputs": [],
      "source": [
        "def lemmatizer(text):\n",
        "  result = []\n",
        "  for word in nlp(text):\n",
        "      result.append(word.lemma_)\n",
        "  return \" \".join(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2nUO4meI7-e"
      },
      "outputs": [],
      "source": [
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGE18lMCHCpg"
      },
      "source": [
        "Estamos testando se mais efetivo utilizar a lemmatização de todo o texto ou apenas os verbos. A lemmatização é para deflexionar a palavra para seu lema.\n",
        "Ex: gatos, gatas, gata são todas do mesmo lema gato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lvlfZ7N9qNW"
      },
      "outputs": [],
      "source": [
        "df['text_lemma'] = df.texto.apply(lemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7VqUVHLI_fK"
      },
      "outputs": [],
      "source": [
        "df['text_lemma_verbs'] = df.texto.apply(lemmatizer_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_-Xo5C8-PEK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWg1CID7mfM-"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOdhx_hFHd7e"
      },
      "source": [
        "Unindo as stopwords das bibliotecas NLTK e Spacy para possuir uma quantidade que abrange mais palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCM95KiUjZ-M"
      },
      "outputs": [],
      "source": [
        "stop_words = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Ycw6Q5HyhB"
      },
      "source": [
        "Vetorização do documento\n",
        "Estamos utilizando uma combinação de unigrama e bigrama com Gradient Boosting\n",
        "\n",
        "O primeiro teste é com CountVectorizer e depois TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frM7Yd-47IDI"
      },
      "outputs": [],
      "source": [
        "#vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=stop_words)\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
        "#vectorizer = CountVectorizer(ngram_range=(2,2), stop_words=stop_words)\n",
        "vectorizer.fit(df_train.text_lemma_verbs)\n",
        "vector = vectorizer.transform(df_train.text_lemma_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gns8aan4jaBd"
      },
      "outputs": [],
      "source": [
        "# vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=stop_words, use_idf=True, norm='l2')\n",
        "# vectorizer.fit(df_train.text_lemma_verbs)\n",
        "# vector = vectorizer.transform(df_train.text_lemma_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25cBRwGAw8-1"
      },
      "outputs": [],
      "source": [
        "gradient_boosting = GradientBoostingClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxWYMVPyjaDl"
      },
      "outputs": [],
      "source": [
        "gradient_boosting.fit(vector, df_train.sentimento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypWmgnpSomG_"
      },
      "outputs": [],
      "source": [
        "vector_test = vectorizer.transform(df_test.text_lemma_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmIH5lYoomOu"
      },
      "outputs": [],
      "source": [
        "predict = gradient_boosting.predict(vector_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDynKfLspBNW"
      },
      "outputs": [],
      "source": [
        "f1_score(df_test.sentimento, predict, average=\"weighted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF com RandomForest, unigrama e bigrama combinados"
      ],
      "metadata": {
        "id": "lzfQwNBwaN_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKPuumYRJmhF"
      },
      "outputs": [],
      "source": [
        "#vectorizer2 = TfidfVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
        "vectorizer2 = CountVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
        "vectorizer2.fit(df_train.text_lemma_verbs)\n",
        "vector2 = vectorizer.transform(df_train.text_lemma_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaZ4aLL_Jmm5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "forest = RandomForestClassifier(random_state=42)\n",
        "forest.fit(vector2, df_train.sentimento)\n",
        "\n",
        "vector_test2 = vectorizer2.transform(df_test.text_lemma_verbs)\n",
        "\n",
        "predict2 = forest.predict(vector_test2)\n",
        "f1_score(df_test.sentimento, predict2, average=\"weighted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigrama e trigrama combinados com DecisionTree"
      ],
      "metadata": {
        "id": "XzSag5y0aSxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy137hrnJmvu"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "vectorizer3 = CountVectorizer(ngram_range=(1,3), stop_words=stop_words)\n",
        "vectorizer3.fit(df_train.text_lemma_verbs)\n",
        "vector3 = vectorizer3.transform(df_train.text_lemma_verbs)\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "tree.fit(vector3, df_train.sentimento)\n",
        "\n",
        "vector_test3 = vectorizer3.transform(df_test.text_lemma_verbs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77M9x8h2tL6G"
      },
      "outputs": [],
      "source": [
        "predict3 = tree.predict(vector_test3) \n",
        "f1_score(df_test.sentimento, predict3, average=\"weighted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "####**Validação do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras \n",
        "implementações utilizadas no processo de desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Os textos foram pré-processados com lemmatizer porém apenas para os verbos, \n",
        "para as palavras serem analisadas como um único item (lema da palavras).\n",
        "Unimos as stop words de spacy e nltk.\n",
        "\n",
        "Entre TfidfVectorizer e CountVectorizer, o CountVectorizer apresentou melhor resultado contendo uma contagem de tokens.\n",
        "\n",
        "De todos os testes realizados o algoritmo que melhor trouxe resultado no F1-score foi RandomForest. \n"
      ],
      "metadata": {
        "id": "PDuZ72Dj3g8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFA-CYfawkEJ"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv', encoding='utf-8')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "df['text_lemma_verbs'] = df.texto.apply(lemmatizer_verbs)\n",
        "\n",
        "df_train, df_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "stop_words = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
        "vectorizer.fit(df_train.text_lemma_verbs)\n",
        "vector = vectorizer.transform(df_train.text_lemma_verbs)\n",
        "\n",
        "forest = RandomForestClassifier(random_state=42)\n",
        "forest.fit(vector2, df_train.sentimento)\n",
        "\n",
        "vector_test = vectorizer.transform(df_test.text_lemma_verbs)\n",
        "\n",
        "predict = forest.predict(vector_test)\n",
        "f1_score(df_test.sentimento, predict, average=\"weighted\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FilmeReviewsTrabalhoFinal_NLP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}