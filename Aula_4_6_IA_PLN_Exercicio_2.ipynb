{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 4-6 IA PLN - Exercicio 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g-roger/natural-language-process/blob/main/Aula_4_6_IA_PLN_Exercicio_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoKbO5UyEzRQ"
      },
      "source": [
        "### **Exercício 2**\n",
        "\n",
        "Dado o dataset de produtos [1], descubra:\n",
        "- Treine deferentes modelos de classificação do pacote scikit-learn para classificar os produtos em suas categorias.\n",
        "- Experimente a lib SpaCy para remover as stop words e reduzir as palavras ao seu lema. Veja como essas alterações impactam o desempenho do classificador.\n",
        "- Compare a performance entre eles usando a métrica de acurácia.\n",
        "- Use randon_state igual a 42 para permitir a comparação com seus colegas, separando 30% para teste.\n",
        "\n",
        "\n",
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf0J_WBkE9Dd"
      },
      "source": [
        "Carregando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhkRPtt4mxf9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkBLb7-uVJvt"
      },
      "source": [
        "'''\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8').sample(frac=0.5, random_state=42)\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8').sample(1000)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxh2XTXYVQIk"
      },
      "source": [
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STMWVw3dsBOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa11c4a4-3b7c-4de7-fb3f-183fb05166c4"
      },
      "source": [
        "df.categoria.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "livro        838\n",
              "maquiagem    788\n",
              "brinquedo    668\n",
              "game         622\n",
              "Name: categoria, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm37-nBoFM0d"
      },
      "source": [
        "# função de lematização completa do documento\n",
        "def lemmatizer_text(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      sent.append(word.lemma_)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# função de lematização para os verbos do documento\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A40YWXqWV6cf"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma22awlbFMxu",
        "outputId": "51e561c2-08a9-4b88-9d16-be12a861d9ce"
      },
      "source": [
        "# teste das funções de lematização\n",
        "import spacy\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# validação das funções\n",
        "print(lemmatizer_text('correndo 1, 2, 3'))\n",
        "print(lemmatizer_verbs('correndo 1, 2, 3'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correr 1 , 2 , 3\n",
            "correr 1 , 2 , 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctc2CF4bVjsb"
      },
      "source": [
        "# aplica a lematização no dataframe criando novas colunas\n",
        "df['text_lemma'] = df.texto.apply(lemmatizer_text)\n",
        "df['text_lemma_verbs'] = df.texto.apply(lemmatizer_verbs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmJC_ppIXxwv",
        "outputId": "8f64b289-ef1c-4a6e-905d-86433c5027bb"
      },
      "source": [
        "# análise para comparação dos textos\n",
        "print(df['texto'][0])\n",
        "print(df['text_lemma'][0])\n",
        "print(df['text_lemma_verbs'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " O Hobbit - 7ª Ed. 2013  Produto NovoBilbo Bolseiro é um hobbit que leva uma vida confortável e sem ambições. Mas seu contentamento é perturbado quando Gandalf, o mago, e uma companhia de anões batem à sua porta e levam-no para uma expedição. Eles têm um plano para roubar o tesouro guardado por Smaug, o Magnífico, um grande e perigoso dragão. Bilbo reluta muito em participar da aventura, mas acaba surpreendendo até a si mesmo com sua esperteza e sua habilidade como ladrão!CaracterísticasAutor: Tolkien, J. R. R.Peso: 0.44I.S.B.N.: 9788578277109Altura: 20.000000Largura: 13.000000Profundidade: 1.000000Número de Páginas: 328Idioma: PortuguêsAcabamento: BrochuraNúmero da edição: 7Ano da edição: 2013\n",
            "  O Hobbit - 7ª Ed . 2013   Produto NovoBilbo Bolseiro ser um hobbit que levar umar vidar confortável e sem ambição . Mas seu contentamento ser perturbar quando Gandalf , o mago , e umar companhia de anão bater à suar portar e levam-no parir umar expedição . Eles ter um planar parir roubar o tesourar guardar por Smaug , o Magnífico , um grande e perigoso dragão . Bilbo relutar muito em participar da aventurar , mas acabar surpreender até o si mesmo com suar esperteza e suar habilidade comer ladrão!CaracterísticasAutor : Tolkien , J. R. R.Peso : 0.44I.S.B.N. : 9788578277109Altura : 20.000000Largura : 13.000000Profundidade : 1.000000Número de Páginas : 328Idioma : PortuguêsAcabamento : BrochuraNúmero da edição : 7Ano da edição : 2013\n",
            "  O Hobbit - 7ª Ed . 2013   Produto NovoBilbo Bolseiro ser um hobbit que levar uma vida confortável e sem ambições . Mas seu contentamento é perturbar quando Gandalf , o mago , e uma companhia de anões bater à sua porta e levam-no para uma expedição . Eles ter um plano para roubar o tesouro guardar por Smaug , o Magnífico , um grande e perigoso dragão . Bilbo relutar muito em participar da aventura , mas acaba surpreender até a si mesmo com sua esperteza e sua habilidade como ladrão!CaracterísticasAutor : Tolkien , J. R. R.Peso : 0.44I.S.B.N. : 9788578277109Altura : 20.000000Largura : 13.000000Profundidade : 1.000000Número de Páginas : 328Idioma : PortuguêsAcabamento : BrochuraNúmero da edição : 7Ano da edição : 2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRXgcvfsVjqs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Divisão das amostras em treino e teste\n",
        "df_treino, df_teste = train_test_split(\n",
        "      df, \n",
        "      test_size = 0.3, \n",
        "      random_state = 42\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF7H3DdqyZtP"
      },
      "source": [
        "Árvore de decisão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj6Q7nclX9GL",
        "outputId": "a7a35f29-49e3-4aed-e5c6-e453dc15b8c0"
      },
      "source": [
        "# Exemplo 1: vetorização por contagem de termos simples com a combinação de unigrama e bigrama no documento lematizado, sem stopwords do SpaCy e modelo de classificação DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "\n",
        "# stopwords SpaCy\n",
        "nlp = spacy.load('pt')\n",
        "stops = nlp.Defaults.stop_words\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # exemplo 1.1: vetorização e combinação de unigrama sem stopwords\n",
        "#vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # exemplo 1.2: vetorização e combinação de unigrama e bigrama sem stopwords\n",
        "vect.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma)\n",
        "\n",
        "# treinamento do modelo ávore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.text_lemma)\n",
        "predito = tree.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2041, 25832)\n",
            "0.9542857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOapgBKwZ9ao"
      },
      "source": [
        "Exemplo 1.1: 0.9542857142857143\n",
        "\n",
        "Exemplo 1.2: 0.9417142857142857"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ULoTRBl54D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f64f620-0312-40f9-91ae-89a3c0c4bf1d"
      },
      "source": [
        "# Exemplo 2: Vetorização por contagem de termos simples com a combinação de unigrama e bigrama no documento com verbos lematizado, sem stopwords do SpaCy e modelo de classificação DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "\n",
        "# stopwords SpaCy\n",
        "nlp = spacy.load('pt')\n",
        "stops = nlp.Defaults.stop_words\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # exemplo 2.1: vetorização e combinação de unigrama sem stopwords\n",
        "#vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # exemplo 2.2: vetorização e combinação de unigrama e bigrama sem stopwords\n",
        "vect.fit(df_treino.text_lemma_verbs)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbs)\n",
        "\n",
        "# treinamento do modelo ávore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.text_lemma_verbs)\n",
        "predito = tree.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2041, 27553)\n",
            "0.9588571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm9IOnTex5TS"
      },
      "source": [
        "Exemplo 2.1: 0.9588571428571429\n",
        "\n",
        "Exemplo 2.2: 0.9542857142857143"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb75D1pP0vop",
        "outputId": "03642fd9-fea3-4b7d-bbc0-9970113061d0"
      },
      "source": [
        "# Exemplo 3: Vetorização por contagem de termos TF-IDF com a combinação de unigrama com documentos lematizado, sem stopwords do SpaCy e NLTK combinadas e modelo de classificação DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops_spacy = nlp.Defaults.stop_words\n",
        "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "stops = list(set(stops_spacy).union(set(stops_nltk)))\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vetorTfidf = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops, norm='l2') # exemplo 3.1: vetorização tf-idf e combinação de unigrama sem stopwords NLTK e Spacy\n",
        "#vetorTfidf = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops_spacy, norm='l2') # exemplo 3.2: vetorização tf-idf e combinação de unigrama e bigrama sem stopwords NLTK e Spacy\n",
        "#vetorTfidf = TfidfVectorizer(ngram_range=(1,2), use_idf=False, stop_words=stops_spacy, norm='l1') # exemplo 3.3: vetorização tf e combinação de unigrama e bigrama sem stopwords NLTK e Spacy\n",
        "vetorTfidf.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vetorTfidf.transform(df_treino.text_lemma)\n",
        "\n",
        "# treinamento do modelo ávore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vetorTfidf.transform(df_teste.text_lemma)\n",
        "predito = tree.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "(2041, 25808)\n",
            "0.9417142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckmp7p-Q2AZ3"
      },
      "source": [
        "Exemplo 3.1: 0.9417142857142857\n",
        "\n",
        "Exemplo 3.2: 0.9417142857142857\n",
        "\n",
        "Exemplo 3.3: 0.9382857142857143"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Rl0sul2iKM",
        "outputId": "805d0b79-982f-4963-a489-3f956d17cc98"
      },
      "source": [
        "# Exemplo 4: Vetorização por contagem de termos simples com a combinação de unigrama e bigrama, documentos com verbos lematizado, sem stopwords do SpaCy e NLTK combinadas e modelo de classificação DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "#vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama, remoção das stopwords NLTK e Spacy\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama e bigrama, remoção das stopwords NLTK e Spacy\n",
        "vect.fit(df_treino.text_lemma_verbs)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbs)\n",
        "\n",
        "# treinamento do modelo ávore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.text_lemma_verbs)\n",
        "predito = tree.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)\n",
        "\n",
        "# Nosso melhor modelo até aqui!!! :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "(2041, 131628)\n",
            "0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po9CmMly2YGH"
      },
      "source": [
        "Exemplo 4.1: 0.9577142857142857\n",
        "\n",
        "Exemplo 4.2: 0.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pErJa-vS49Cw"
      },
      "source": [
        "Regressão Logística"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJEQ4jiVBVXN",
        "outputId": "9e95b56a-371c-4a67-dd6d-8087188f1c58"
      },
      "source": [
        "# Exemplo Base: Vetorização por contagem de termos simples com a combinação de unigramas e modelo de Regressão Logística\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vect = CountVectorizer()\n",
        "vect.fit(df_treino.texto)\n",
        "text_vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.texto)\n",
        "predito = model.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2041, 29957)\n",
            "0.9805714285714285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvG29wzT5Xgq",
        "outputId": "ef9b06fa-2be6-40b9-83d1-b138137ecb17"
      },
      "source": [
        "# Exemplo 5: Vetorização por contagem de termos simples com a combinação de unigramas e bigramas, remoção de stopwords NLTK e SpaCy combinadas, texto com verbos lematizados e modelo de Regressão Logística\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama, remoção das stopwords NLTK e Spacy\n",
        "#vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama e bigrama, remoção das stopwords NLTK e Spacy\n",
        "vect.fit(df_treino.text_lemma_verbs)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbs)\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.text_lemma_verbs)\n",
        "predito = model.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "(2041, 27519)\n",
            "0.9874285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvPod5M66mU-"
      },
      "source": [
        "Exemplo 5.1: 0.9874285714285714\n",
        "\n",
        "Exemplo 5.2: 0.9874285714285714"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5v-dIKo7nyA",
        "outputId": "9f4c0859-5f1d-4f96-a3ed-cb1c967c3197"
      },
      "source": [
        "# Exemplo 6: Vetorização por TF-IDF com a combinação de unigramas, remoção de stopwords NLTK e SpaCy combinadas, texto com verbos lematizados e modelo de Regressão Logística\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama, remoção das stopwords NLTK e Spacy\n",
        "vect.fit(df_treino.text_lemma_verbs)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbs)\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.text_lemma_verbs)\n",
        "predito = model.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "(2041, 27519)\n",
            "0.9851428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Hsfxji7w37",
        "outputId": "373c5e95-8358-498b-9569-53e9982078d8"
      },
      "source": [
        "# Exemplo 7: Vetorização por TF-IDF com a combinação de unigramas e bigramas, remoção de stopwords NLTK e SpaCy combinadas, texto lematizado e modelo de Regressão Logística\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "# vetorização por contagem de termos no documento lematizado\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama, remoção das stopwords NLTK e Spacy\n",
        "#vect = TfidfVectorizer(ngram_range=(1,2), stop_words=stops) # vetorização por contagem de termos, combinação de unigrama e bigrama, remoção das stopwords NLTK e Spacy\n",
        "#vect = TfidfVectorizer(ngram_range=(1,1)) # vetorização por contagem de termos, combinação de unigrama\n",
        "vect.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma)\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "text_vect_teste = vect.transform(df_teste.text_lemma)\n",
        "predito = model.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "accuracy = accuracy_score(predito, df_teste[\"categoria\"])\n",
        "\n",
        "print(text_vect_treino.shape)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "(2041, 25808)\n",
            "0.9828571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvg5bPc27ns3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}